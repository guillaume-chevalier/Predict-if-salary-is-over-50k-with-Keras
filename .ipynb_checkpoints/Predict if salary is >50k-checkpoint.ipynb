{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: [1, 8, 1, 16, 1, 7, 14, 6, 5, 2, 1, 1, 1, 41]\n",
      "input_dim: 105\n",
      "\n",
      "output_dim: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Init global infos\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "inputs = (\n",
    "    (\"age\", (\"continuous\",)), \n",
    "    (\"workclass\", (\"Private\", \"Self-emp-not-inc\", \"Self-emp-inc\", \"Federal-gov\", \"Local-gov\", \"State-gov\", \"Without-pay\", \"Never-worked\")), \n",
    "    (\"fnlwgt\", (\"continuous\",)), \n",
    "    (\"education\", (\"Bachelors\", \"Some-college\", \"11th\", \"HS-grad\", \"Prof-school\", \"Assoc-acdm\", \"Assoc-voc\", \"9th\", \"7th-8th\", \"12th\", \"Masters\", \"1st-4th\", \"10th\", \"Doctorate\", \"5th-6th\", \"Preschool\")), \n",
    "    (\"education-num\", (\"continuous\",)), \n",
    "    (\"marital-status\", (\"Married-civ-spouse\", \"Divorced\", \"Never-married\", \"Separated\", \"Widowed\", \"Married-spouse-absent\", \"Married-AF-spouse\")), \n",
    "    (\"occupation\", (\"Tech-support\", \"Craft-repair\", \"Other-service\", \"Sales\", \"Exec-managerial\", \"Prof-specialty\", \"Handlers-cleaners\", \"Machine-op-inspct\", \"Adm-clerical\", \"Farming-fishing\", \"Transport-moving\", \"Priv-house-serv\", \"Protective-serv\", \"Armed-Forces\")), \n",
    "    (\"relationship\", (\"Wife\", \"Own-child\", \"Husband\", \"Not-in-family\", \"Other-relative\", \"Unmarried\")), \n",
    "    (\"race\", (\"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\")), \n",
    "    (\"sex\", (\"Female\", \"Male\")), \n",
    "    (\"capital-gain\", (\"continuous\",)), \n",
    "    (\"capital-loss\", (\"continuous\",)), \n",
    "    (\"hours-per-week\", (\"continuous\",)), \n",
    "    (\"native-country\", (\"United-States\", \"Cambodia\", \"England\", \"Puerto-Rico\", \"Canada\", \"Germany\", \"Outlying-US(Guam-USVI-etc)\", \"India\", \"Japan\", \"Greece\", \"South\", \"China\", \"Cuba\", \"Iran\", \"Honduras\", \"Philippines\", \"Italy\", \"Poland\", \"Jamaica\", \"Vietnam\", \"Mexico\", \"Portugal\", \"Ireland\", \"France\", \"Dominican-Republic\", \"Laos\", \"Ecuador\", \"Taiwan\", \"Haiti\", \"Columbia\", \"Hungary\", \"Guatemala\", \"Nicaragua\", \"Scotland\", \"Thailand\", \"Yugoslavia\", \"El-Salvador\", \"Trinadad&Tobago\", \"Peru\", \"Hong\", \"Holand-Netherlands\"))\n",
    ")\n",
    "\n",
    "input_shape = []\n",
    "for i in inputs:\n",
    "    count = len(i[1 ])\n",
    "    input_shape.append(count)\n",
    "input_dim = sum(input_shape)\n",
    "print(\"input_shape:\", input_shape)\n",
    "print(\"input_dim:\", input_dim)\n",
    "print()\n",
    "\n",
    "\n",
    "outputs = (0, 1)  # (\">50K\", \"<=50K\")\n",
    "output_dim = 2  # len(outputs)\n",
    "print(\"output_dim:\", output_dim)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load and prepare data\n",
    "\n",
    "def isFloat(string):\n",
    "    # credits: http://stackoverflow.com/questions/2356925/how-to-check-whether-string-might-be-type-cast-to-float-in-python\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def find_means_for_continuous_types(X):\n",
    "    means = []\n",
    "    for col in range(len(X[0])):\n",
    "        summ = 0\n",
    "        count = 0.000000000000000000001\n",
    "        for value in X[:, col]:\n",
    "            if isFloat(value): \n",
    "                summ += float(value)\n",
    "                count +=1\n",
    "        means.append(summ/count)\n",
    "    return means\n",
    "\n",
    "def prepare_data(raw_data, means):\n",
    "    \n",
    "    X = raw_data[:, :-1]\n",
    "    y = raw_data[:, -1:]\n",
    "    \n",
    "    # X:\n",
    "    def flatten_persons_inputs_for_model(person_inputs):\n",
    "        global inputs\n",
    "        global input_shape\n",
    "        global input_dim\n",
    "        global means\n",
    "        float_inputs = []\n",
    "\n",
    "        for i in range(len(input_shape)):\n",
    "            features_of_this_type = input_shape[i]\n",
    "            is_feature_continuous = features_of_this_type == 1\n",
    "\n",
    "            if is_feature_continuous:\n",
    "                mean = means[i]\n",
    "                if isFloat(person_inputs[i]):\n",
    "                    scale_factor = 1/(2*mean)  # we prefer inputs mainly scaled from -1 to 1. \n",
    "                    float_inputs.append(float(person_inputs[i])*scale_factor)\n",
    "                else:\n",
    "                    float_inputs.append(mean)\n",
    "            else:\n",
    "                for j in range(features_of_this_type):\n",
    "                    feature_name = inputs[i][1][j]\n",
    "\n",
    "                    if feature_name == person_inputs[i]:\n",
    "                        float_inputs.append(1.)\n",
    "                    else:\n",
    "                        float_inputs.append(0)\n",
    "        return float_inputs\n",
    "    \n",
    "    new_X = []\n",
    "    for person in range(len(X)):\n",
    "        formatted_X = flatten_persons_inputs_for_model(X[person])\n",
    "        new_X.append(formatted_X)\n",
    "    new_X = np.array(new_X)\n",
    "    \n",
    "    # y:\n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == \">50k\":\n",
    "            new_y.append((1, 0))\n",
    "        else:  # y[i] == \"<=50k\":\n",
    "            new_y.append((0, 1))\n",
    "    new_y = np.array(new_y)\n",
    "    \n",
    "    return (new_X, new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count: 32561\n",
      "Test data count: 16281\n",
      "Mean values for data types (if continuous): [38.64358543876172, 0.0, 189664.13459727284, 0.0, 10.078088530363212, 0.0, 0.0, 0.0, 0.0, 0.0, 1079.0676262233324, 87.50231358257237, 40.422382375824085, 0.0, 0.0]\n",
      "Training data percentage that is >50k: 24.0809557446 %\n"
     ]
    }
   ],
   "source": [
    "# Building training and test data\n",
    "\n",
    "training_data = np.genfromtxt('data/adult.data.txt', delimiter=', ', dtype=str, autostrip=True)\n",
    "print(\"Training data count:\", len(training_data))\n",
    "test_data = np.genfromtxt('data/adult.test.txt', delimiter=', ', dtype=str, autostrip=True)\n",
    "print(\"Test data count:\", len(test_data))\n",
    "\n",
    "means = find_means_for_continuous_types(np.concatenate((training_data, test_data), 0))\n",
    "print(\"Mean values for data types (if continuous):\", means)\n",
    "\n",
    "X_train, y_train = prepare_data(training_data, means)\n",
    "X_test, y_test = prepare_data(test_data, means)\n",
    "\n",
    "percent = sum([i[0] for i in y_train])/len(y_train)\n",
    "print(\"Training data percentage that is >50k:\", percent*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data format example:\n",
      "[ 0.36228522  1.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.89212702  1.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.64496357\n",
      "  1.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          1.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          1.          1.          0.          0.\n",
      "  0.          0.49477539  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.        ]\n",
      "\n",
      "In fact, we just crushed the data in such a way that it will optimise the neural network (model). \n",
      "It is crushed according to the `input_shape` variable: \n",
      "    say, if there are 41 native countries in the dataset, there will be 41 input dimensions for the \n",
      "    neural network with a value of 0 for every 41 input node for a given person, except that the \n",
      "    node representing the real country of the person will have a value of 1.\n"
     ]
    }
   ],
   "source": [
    "# Explanation on data format\n",
    "\n",
    "print(\"Training data format example:\")\n",
    "print(X_train[4])  # 4 is a random person, from cuba. \n",
    "print()\n",
    "\n",
    "print(\"In fact, we just crushed the data in such a way that it will optimise the neural network (model). \\n\\\n",
    "It is crushed according to the `input_shape` variable: \\n\\\n",
    "    say, if there are 41 native countries in the dataset, there will be 41 input dimensions for the \\n\\\n",
    "    neural network with a value of 0 for every 41 input node for a given person, except that the \\n\\\n",
    "    node representing the real country of the person will have a value of 1.\")\n",
    "\n",
    "for i in X_train:\n",
    "    if len(i) != input_dim:\n",
    "        raise Exception(\n",
    "            \"Every person should have 105 data fields now. {} here.\".format(len(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Init model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(output_dim=output_dim, activation='sigmoid', input_dim=input_dim))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(training_datas, dimension): (32561, 105)\n",
      "Train on 29304 samples, validate on 3257 samples\n",
      "Epoch 0\n",
      "29304/29304 [==============================] - 0s - loss: 0.5197 - acc: 0.7881 - val_loss: 0.4435 - val_acc: 0.8072\n",
      "Epoch 1\n",
      "29304/29304 [==============================] - 0s - loss: 0.4077 - acc: 0.8149 - val_loss: 0.3836 - val_acc: 0.8247\n",
      "Epoch 2\n",
      "29304/29304 [==============================] - 0s - loss: 0.3701 - acc: 0.8323 - val_loss: 0.3592 - val_acc: 0.8422\n",
      "Epoch 3\n",
      "29304/29304 [==============================] - 0s - loss: 0.3537 - acc: 0.8425 - val_loss: 0.3481 - val_acc: 0.8416\n",
      "Epoch 4\n",
      "29304/29304 [==============================] - 0s - loss: 0.3453 - acc: 0.8457 - val_loss: 0.3414 - val_acc: 0.8471\n",
      "Epoch 5\n",
      "29304/29304 [==============================] - 0s - loss: 0.3404 - acc: 0.8465 - val_loss: 0.3376 - val_acc: 0.8468\n",
      "Epoch 6\n",
      "29304/29304 [==============================] - 0s - loss: 0.3372 - acc: 0.8472 - val_loss: 0.3345 - val_acc: 0.8471\n",
      "Epoch 7\n",
      "29304/29304 [==============================] - 0s - loss: 0.3348 - acc: 0.8481 - val_loss: 0.3327 - val_acc: 0.8480\n",
      "Epoch 8\n",
      "29304/29304 [==============================] - 0s - loss: 0.3331 - acc: 0.8482 - val_loss: 0.3314 - val_acc: 0.8474\n",
      "Epoch 9\n",
      "29304/29304 [==============================] - 0s - loss: 0.3318 - acc: 0.8483 - val_loss: 0.3302 - val_acc: 0.8474\n",
      "Epoch 10\n",
      "29304/29304 [==============================] - 0s - loss: 0.3307 - acc: 0.8484 - val_loss: 0.3293 - val_acc: 0.8477\n",
      "Epoch 11\n",
      "29304/29304 [==============================] - 0s - loss: 0.3298 - acc: 0.8485 - val_loss: 0.3286 - val_acc: 0.8465\n",
      "Epoch 12\n",
      "29304/29304 [==============================] - 0s - loss: 0.3290 - acc: 0.8484 - val_loss: 0.3279 - val_acc: 0.8459\n",
      "Epoch 13\n",
      "29304/29304 [==============================] - 0s - loss: 0.3283 - acc: 0.8483 - val_loss: 0.3275 - val_acc: 0.8459\n",
      "Epoch 14\n",
      "29304/29304 [==============================] - 0s - loss: 0.3277 - acc: 0.8489 - val_loss: 0.3268 - val_acc: 0.8446\n",
      "Epoch 15\n",
      "29304/29304 [==============================] - 0s - loss: 0.3272 - acc: 0.8495 - val_loss: 0.3266 - val_acc: 0.8468\n",
      "Epoch 16\n",
      "29304/29304 [==============================] - 0s - loss: 0.3267 - acc: 0.8499 - val_loss: 0.3262 - val_acc: 0.8465\n",
      "Epoch 17\n",
      "29304/29304 [==============================] - 0s - loss: 0.3262 - acc: 0.8503 - val_loss: 0.3257 - val_acc: 0.8446\n",
      "Epoch 18\n",
      "29304/29304 [==============================] - 0s - loss: 0.3258 - acc: 0.8504 - val_loss: 0.3253 - val_acc: 0.8459\n",
      "Epoch 19\n",
      "29304/29304 [==============================] - 0s - loss: 0.3254 - acc: 0.8504 - val_loss: 0.3251 - val_acc: 0.8459\n",
      "Epoch 20\n",
      "29304/29304 [==============================] - 0s - loss: 0.3250 - acc: 0.8504 - val_loss: 0.3251 - val_acc: 0.8453\n",
      "Epoch 21\n",
      "29304/29304 [==============================] - 0s - loss: 0.3248 - acc: 0.8504 - val_loss: 0.3247 - val_acc: 0.8446\n",
      "Epoch 22\n",
      "29304/29304 [==============================] - 0s - loss: 0.3244 - acc: 0.8508 - val_loss: 0.3246 - val_acc: 0.8459\n",
      "Epoch 23\n",
      "29304/29304 [==============================] - 0s - loss: 0.3242 - acc: 0.8510 - val_loss: 0.3243 - val_acc: 0.8446\n",
      "Epoch 24\n",
      "29304/29304 [==============================] - 0s - loss: 0.3238 - acc: 0.8511 - val_loss: 0.3240 - val_acc: 0.8456\n",
      "Epoch 25\n",
      "29304/29304 [==============================] - 0s - loss: 0.3236 - acc: 0.8519 - val_loss: 0.3242 - val_acc: 0.8468\n",
      "Epoch 26\n",
      "29304/29304 [==============================] - 0s - loss: 0.3234 - acc: 0.8511 - val_loss: 0.3239 - val_acc: 0.8471\n",
      "Epoch 27\n",
      "29304/29304 [==============================] - 0s - loss: 0.3231 - acc: 0.8519 - val_loss: 0.3237 - val_acc: 0.8459\n",
      "Epoch 28\n",
      "29304/29304 [==============================] - 0s - loss: 0.3230 - acc: 0.8517 - val_loss: 0.3237 - val_acc: 0.8462\n",
      "Epoch 29\n",
      "29304/29304 [==============================] - 0s - loss: 0.3227 - acc: 0.8517 - val_loss: 0.3233 - val_acc: 0.8468\n",
      "Epoch 30\n",
      "29304/29304 [==============================] - 0s - loss: 0.3225 - acc: 0.8523 - val_loss: 0.3232 - val_acc: 0.8471\n",
      "Epoch 31\n",
      "29304/29304 [==============================] - 0s - loss: 0.3223 - acc: 0.8518 - val_loss: 0.3232 - val_acc: 0.8468\n",
      "Epoch 32\n",
      "29304/29304 [==============================] - 0s - loss: 0.3221 - acc: 0.8523 - val_loss: 0.3232 - val_acc: 0.8477\n",
      "Epoch 33\n",
      "29304/29304 [==============================] - 0s - loss: 0.3220 - acc: 0.8528 - val_loss: 0.3228 - val_acc: 0.8471\n",
      "Epoch 34\n",
      "29304/29304 [==============================] - 0s - loss: 0.3218 - acc: 0.8523 - val_loss: 0.3227 - val_acc: 0.8465\n",
      "Epoch 35\n",
      "29304/29304 [==============================] - 0s - loss: 0.3216 - acc: 0.8516 - val_loss: 0.3228 - val_acc: 0.8477\n",
      "Epoch 36\n",
      "29304/29304 [==============================] - 0s - loss: 0.3215 - acc: 0.8529 - val_loss: 0.3225 - val_acc: 0.8477\n",
      "Epoch 37\n",
      "29304/29304 [==============================] - 0s - loss: 0.3213 - acc: 0.8526 - val_loss: 0.3224 - val_acc: 0.8477\n",
      "Epoch 38\n",
      "29304/29304 [==============================] - 0s - loss: 0.3212 - acc: 0.8521 - val_loss: 0.3225 - val_acc: 0.8474\n",
      "Epoch 39\n",
      "29304/29304 [==============================] - 0s - loss: 0.3210 - acc: 0.8527 - val_loss: 0.3222 - val_acc: 0.8483\n",
      "Epoch 40\n",
      "29304/29304 [==============================] - 0s - loss: 0.3209 - acc: 0.8527 - val_loss: 0.3223 - val_acc: 0.8477\n",
      "Epoch 41\n",
      "29304/29304 [==============================] - 0s - loss: 0.3208 - acc: 0.8529 - val_loss: 0.3221 - val_acc: 0.8483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f55cfeb21d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "print(\"(training_datas, dimension):\", X_train.shape)\n",
    "# model.fit(new_X_train, y_train, nb_epoch=3, batch_size=16, show_accuracy=True, verbose=2)\n",
    "model.fit(X_train, y_train, nb_epoch=42, batch_size=128, validation_split=0.1, show_accuracy=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16281/16281 [==============================] - 0s     \n",
      "\n",
      "Test Results for on 16281 test entries on which we did not trained the neural network.\n",
      "\n",
      "Keras evaluation result: 0.320646480683\n",
      "Percentage right: 85.19746944290891%.\n",
      "Error: 14.802530557091087%.\n",
      "\n",
      "Confusion matrix:\n",
      "[[23091  3169]\n",
      " [ 1629  4672]]\n",
      "Confusion matrix, percentage of data:\n",
      "[[ 70.91612665   9.73250207]\n",
      " [  5.0029176   14.34845367]]\n",
      "Confusion matrix interpretation:\n",
      " [['true negative' 'false negative']\n",
      " ['false positive' 'true positive']]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate training\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1, show_accuracy=True)\n",
    "print(\"\\nTest Results for on {} test entries \\\n",
    "on which we did not trained the neural network.\\n\".format(len(X_test)))\n",
    "\n",
    "print(\"Keras evaluation result:\", score[0])\n",
    "\n",
    "print(\"Percentage right: {}%.\".format(score[1]*100))\n",
    "print(\"Error: {}%.\\n\".format((1-score[1])*100))\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    confusion_matrix = np.array([\n",
    "        [0, 0], \n",
    "        [0, 0]\n",
    "    ])\n",
    "    pred = model.predict(X_train)\n",
    "    for i in range(len(pred)):\n",
    "        prediction = pred[i]\n",
    "        if prediction[0]>prediction[1]:\n",
    "            prediction = 1\n",
    "        else:\n",
    "            prediction = 0\n",
    "\n",
    "        expected = y_train[i][0]\n",
    "\n",
    "        confusion_matrix[prediction][expected] += 1\n",
    "    \n",
    "    return confusion_matrix\n",
    "\n",
    "confusion_matrix = evaluate_model(model, X_test, y_test)\n",
    "confusion_matrix_interpretation = np.array([\n",
    "        [\"true negative\", \"false negative\"], \n",
    "        [\"false positive\", \"true positive\"]\n",
    "    ])\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix)\n",
    "print(\"Confusion matrix, percentage of data:\")\n",
    "print(confusion_matrix*100/sum(confusion_matrix.flatten()))\n",
    "print(\"Confusion matrix interpretation:\\n\", confusion_matrix_interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
